# src/services/enhanced_patent_qa_service.py - æ”¯æŒå°è©±è¨˜æ†¶å’Œå¤šé‡æœå°‹çµæœçš„å•ç­”æœå‹™

import asyncio
import aiohttp
import logging
import json
import re
import time
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from datetime import datetime

from src.database import DatabaseManager
from src.config import settings

logger = logging.getLogger(__name__)

class ConversationManager:
    """å°è©±ç®¡ç†å™¨ - è™•ç†å°è©±æ­·å²å’Œtokenæ§åˆ¶"""
    
    def __init__(self, max_tokens: int = 128000):
        self.max_tokens = max_tokens
        self.system_prompt_tokens = 500  # é ä¼°ç³»çµ±æç¤ºè©tokenæ•¸
        self.context_tokens = 2000       # é ä¼°å°ˆåˆ©ä¸Šä¸‹æ–‡tokenæ•¸  
        self.response_tokens = 1000      # é ç•™å›æ‡‰tokenæ•¸
        self.safety_margin = 1000        # å®‰å…¨é‚Šç•Œ
        
        # å¯ç”¨æ–¼å°è©±æ­·å²çš„tokenæ•¸
        self.available_history_tokens = (
            self.max_tokens - 
            self.system_prompt_tokens - 
            self.context_tokens - 
            self.response_tokens - 
            self.safety_margin
        )
    
    def estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬tokenæ•¸é‡ï¼ˆç°¡åŒ–æ–¹æ³•ï¼‰"""
        # ä¸­æ–‡ç´„1.5å€‹å­—ç¬¦=1å€‹tokenï¼Œè‹±æ–‡ç´„4å€‹å­—ç¬¦=1å€‹token
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(text) - chinese_chars
        
        estimated_tokens = (chinese_chars / 1.5) + (english_chars / 4)
        return int(estimated_tokens * 1.2)  # åŠ 20%å®‰å…¨é‚Šç•Œ
    
    def trim_conversation_history(self, history: List[Dict]) -> List[Dict]:
        """ä¿®å‰ªå°è©±æ­·å²ä»¥é©æ‡‰tokené™åˆ¶"""
        if not history:
            return []
        
        trimmed_history = []
        current_tokens = 0
        
        # å¾æœ€æ–°çš„å°è©±é–‹å§‹å¾€å‰æ·»åŠ 
        for item in reversed(history):
            question_tokens = self.estimate_tokens(item.get('question', ''))
            answer_tokens = self.estimate_tokens(item.get('answer', ''))
            total_tokens = question_tokens + answer_tokens
            
            if current_tokens + total_tokens <= self.available_history_tokens:
                current_tokens += total_tokens
                trimmed_history.insert(0, item)  # æ’å…¥åˆ°é–‹é ­ä¿æŒé †åº
            else:
                break
        
        logger.info(f"å°è©±æ­·å²ä¿®å‰ª: ä¿ç•™ {len(trimmed_history)}/{len(history)} è¼ªå°è©±ï¼Œç´„ {current_tokens} tokens")
        return trimmed_history
    
    def build_messages_with_history(self, 
                                  system_prompt: str,
                                  context: str, 
                                  current_question: str,
                                  history: List[Dict]) -> List[Dict]:
        """æ§‹å»ºåŒ…å«å°è©±æ­·å²çš„messages"""
        messages = [
            {
                "role": "system",
                "content": system_prompt + f"\n\nå°ˆåˆ©æª¢ç´¢çµæœï¼š\n{context}"
            }
        ]
        
        # æ·»åŠ ä¿®å‰ªå¾Œçš„å°è©±æ­·å²
        trimmed_history = self.trim_conversation_history(history)
        
        for item in trimmed_history:
            messages.append({
                "role": "user",
                "content": item['question']
            })
            messages.append({
                "role": "assistant", 
                "content": item['answer']
            })
        
        # æ·»åŠ ç•¶å‰å•é¡Œ
        messages.append({
            "role": "user",
            "content": current_question
        })
        
        return messages

class EnhancedPatentQAService:
    """å¢å¼·ç‰ˆå°ˆåˆ©å•ç­”æœå‹™ - æ”¯æŒå°è©±è¨˜æ†¶å’Œå¤šé‡æœå°‹çµæœ"""
    
    def __init__(self):
        self.qwen_api_url = settings.QWEN_API_URL
        self.qwen_model = settings.QWEN_MODEL
        self.session = None
        self.conversation_manager = ConversationManager(max_tokens=128000)
        
        # æœƒè©±å°è©±ç·©å­˜ï¼ˆå…§å­˜ä¸­ï¼‰
        self.session_conversations = {}
        
    async def initialize(self):
        """åˆå§‹åŒ–HTTPæœƒè©±"""
        if self.session is None:
            timeout = aiohttp.ClientTimeout(total=120.0)
            self.session = aiohttp.ClientSession(timeout=timeout)
            logger.info("å¢å¼·ç‰ˆå•ç­”æœå‹™å·²åˆå§‹åŒ–ï¼ˆæ”¯æŒå°è©±è¨˜æ†¶å’Œå¤šé‡æœå°‹çµæœï¼‰")
    
    async def close(self):
        """é—œé–‰HTTPæœƒè©±"""
        if self.session:
            await self.session.close()
            self.session = None
    
    async def answer_question_with_memory(
        self, 
        session_id: str, 
        question: str, 
        context_patents: List[Dict] = None,
        use_memory: bool = True
    ) -> Dict:
        """
        å›ç­”å•é¡Œä¸¦æ”¯æŒå°è©±è¨˜æ†¶ - æ”¯æ´å¤šé‡æœå°‹çµæœç‰ˆæœ¬

        Args:
            session_id: æœƒè©±ID
            question: ç”¨æˆ¶å•é¡Œ
            context_patents: ä¸Šä¸‹æ–‡å°ˆåˆ©æ•¸æ“š
            use_memory: æ˜¯å¦ä½¿ç”¨å°è©±è¨˜æ†¶
        """
        start_time = time.time()

        try:
            logger.info(f"ğŸ¤– è™•ç†å•ç­”è«‹æ±‚ï¼ˆè¨˜æ†¶æ¨¡å¼: {use_memory}ï¼‰: {session_id}")
            logger.info(f"â“ å•é¡Œ: {question}")

            # ğŸ†• æª¢æŸ¥å¯ç”¨çš„æœå°‹é¡å‹
            available_types = await DatabaseManager.get_available_search_types(session_id)

            if not available_types:
                return {
                    'success': True,
                    'answer': self._generate_no_search_results_response(question),
                    'context_info': {
                        'memory_enabled': use_memory,
                        'has_search_cache': False,
                        'search_results_count': 0,
                        'available_search_types': [],
                        'response_type': 'no_search_data_guidance'
                    },
                    'session_id': session_id
                }
        
            # ğŸ†• æ™ºèƒ½åˆ¤æ–·ç”¨æˆ¶æƒ³è©¢å•å“ªç¨®æœå°‹çµæœ
            target_search_type = self._determine_target_search_type(question, available_types)

            # ğŸ†• ç²å–å°æ‡‰çš„æœå°‹çµæœ
            if target_search_type:
                context_patents = await DatabaseManager.get_cached_search_results_by_type(
                    session_id, target_search_type
                )
            else:
                # å¦‚æœç„¡æ³•åˆ¤æ–·ï¼Œç²å–æ‰€æœ‰çµæœ
                context_patents = await DatabaseManager.get_cached_search_results_by_type(session_id)

            if not context_patents:
                return {
                    'success': False,
                    'answer': 'æŠ±æ­‰ï¼Œæ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„å°ˆåˆ©æ•¸æ“šã€‚è«‹å…ˆé€²è¡Œå°ˆåˆ©æª¢ç´¢ã€‚',
                    'error': 'No cached patents found'
                }

            # ç²å–å°è©±æ­·å²
            conversation_history = []
            if use_memory:
                # å…ˆå¾å…§å­˜ç·©å­˜ç²å–
                if session_id in self.session_conversations:
                    conversation_history = self.session_conversations[session_id]
                else:
                    # å¾æ•¸æ“šåº«ç²å–
                    db_history = await DatabaseManager.get_qa_history(session_id, limit=20)
                    conversation_history = db_history
                    # ç·©å­˜åˆ°å…§å­˜
                    self.session_conversations[session_id] = conversation_history

            # è§£æå•é¡Œï¼Œç¢ºå®šéœ€è¦å¼•ç”¨çš„å°ˆåˆ©
            referenced_patents = self._extract_patent_references(question, context_patents)

            # ğŸ†• æ§‹å»ºå¤šé‡æœå°‹ä¸Šä¸‹æ–‡
            context = self._build_multi_search_context(
                question, context_patents, referenced_patents, target_search_type
            )

            # ğŸ†• å¢å¼·å•é¡Œï¼ŒåŠ å…¥æœå°‹é¡å‹ä¿¡æ¯
            enhanced_question = self._enhance_question_with_search_info(
                question, available_types, target_search_type, len(context_patents)
            )

            # èª¿ç”¨QWEN APIï¼ˆåŒ…å«å°è©±æ­·å²ï¼‰
            if not self.session:
                await self.initialize()

            answer = await self._call_qwen_api_with_memory(
                enhanced_question, 
                context, 
                conversation_history if use_memory else []
            )

            # ğŸ†• åœ¨å›ç­”å¾ŒåŠ å…¥æœå°‹ä¾†æºèªªæ˜
            answer_with_source = self._add_source_info_to_answer(
                answer, available_types, target_search_type, len(context_patents)
            )

            execution_time = time.time() - start_time

            # ä¿å­˜å•ç­”æ­·å²åˆ°æ•¸æ“šåº«
            await DatabaseManager.save_qa_history(
                session_id=session_id,
                question=question,
                answer=answer_with_source,
                referenced_patents=referenced_patents,
                execution_time=execution_time
            )

            # æ›´æ–°å…§å­˜ç·©å­˜
            if use_memory:
                if session_id not in self.session_conversations:
                    self.session_conversations[session_id] = []

                self.session_conversations[session_id].append({
                    'question': question,
                    'answer': answer_with_source,
                    'referenced_patents': referenced_patents,
                    'created_at': datetime.now().isoformat()
                })

                # é™åˆ¶å…§å­˜ç·©å­˜å¤§å°
                if len(self.session_conversations[session_id]) > 50:
                    self.session_conversations[session_id] = self.session_conversations[session_id][-30:]

            logger.info(f"âœ… å•ç­”å®Œæˆï¼ˆä½¿ç”¨è¨˜æ†¶: {use_memory}ï¼‰ï¼Œè€—æ™‚: {execution_time:.2f}ç§’")

            return {
                'success': True,
                'answer': answer_with_source,
                'referenced_patents': referenced_patents,
                'execution_time': execution_time,
                'context_patent_count': len(context_patents),
                'conversation_history_used': len(conversation_history) if use_memory else 0,
                'memory_enabled': use_memory,
                # ğŸ†• æ–°å¢å¤šé‡æœå°‹ç›¸é—œä¿¡æ¯
                'context_info': {
                    'available_search_types': available_types,
                    'target_search_type': target_search_type,
                    'search_results_count': len(context_patents),
                    'response_type': 'normal'
                }
            }

        except Exception as e:
            logger.error(f"âŒ å•ç­”è™•ç†å¤±æ•—: {e}")
            return {
                'success': False,
                'answer': f'æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}',
                'error': str(e)
            }

    async def _call_qwen_api_with_memory(
        self, 
        question: str, 
        context: str,
        conversation_history: List[Dict]
    ) -> str:
        """èª¿ç”¨QWEN APIä¸¦åŒ…å«å°è©±æ­·å²"""
        try:
            # æ§‹å»ºç³»çµ±æç¤ºè©
            system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å°ˆåˆ©æª¢ç´¢åŠ©æ‰‹ã€‚ä½ èƒ½å¤ è¨˜ä½æˆ‘å€‘ä¹‹å‰çš„å°è©±å…§å®¹ï¼Œä¸¦åŸºæ–¼æª¢ç´¢çµæœå’Œå°è©±æ­·å²ä¾†å›ç­”å•é¡Œã€‚

å›ç­”è¦æ±‚ï¼š
1. ç”¨ç¹é«”ä¸­æ–‡å›ç­”
2. åŸºæ–¼æä¾›çš„å°ˆåˆ©æª¢ç´¢çµæœå›ç­”å•é¡Œ
3. å¦‚æœç”¨æˆ¶å•åˆ°ç‰¹å®šå°ˆåˆ©ï¼ˆå¦‚"ç¬¬3ç­†å°ˆåˆ©"ï¼‰ï¼Œè«‹æ ¹æ“šåºè™Ÿæ‰¾åˆ°å°æ‡‰çš„å°ˆåˆ©è³‡æ–™
4. å¦‚æœæ¶‰åŠç¿»è­¯ï¼Œè«‹æä¾›æº–ç¢ºçš„ä¸­è‹±æ–‡å°ç…§
5. å¦‚æœæ˜¯æŠ€è¡“åˆ†æï¼Œè«‹èšç„¦æ–¼æŠ€è¡“ç‰¹å¾µå’ŒåŠŸæ•ˆ
6. è¨˜ä½æˆ‘å€‘ä¹‹å‰è¨è«–çš„å…§å®¹ï¼Œå¯ä»¥åƒè€ƒå‰é¢çš„å°è©±
7. å›ç­”è¦ç°¡æ½”æ˜ç¢ºï¼Œé¿å…å†—é•·
8. å¦‚æœç”¨æˆ¶å•é¡Œè¶…å‡ºæª¢ç´¢çµæœç¯„åœï¼Œè«‹èªªæ˜ç„¡æ³•å›ç­”çš„åŸå› """

            # ä½¿ç”¨å°è©±ç®¡ç†å™¨æ§‹å»ºåŒ…å«æ­·å²çš„messages
            messages = self.conversation_manager.build_messages_with_history(
                system_prompt=system_prompt,
                context=context,
                current_question=question,
                history=conversation_history
            )
            
            # ä¼°ç®—ç¸½tokenæ•¸ç”¨æ–¼æ—¥èªŒ
            total_estimated_tokens = sum(
                self.conversation_manager.estimate_tokens(str(msg)) 
                for msg in messages
            )
            logger.info(f"ğŸ“Š é ä¼°ç¸½tokenæ•¸: {total_estimated_tokens}/128000")
            
            payload = {
                "model": self.qwen_model,
                "messages": messages,
                "temperature": 0.3,
                "max_tokens": 1500,  # å¢åŠ æœ€å¤§å›æ‡‰é•·åº¦
                "stream": False
            }
            
            async with self.session.post(
                f"{self.qwen_api_url}/v1/chat/completions",
                json=payload,
                headers={'Content-Type': 'application/json'}
            ) as response:
                
                if response.status == 200:
                    data = await response.json()
                    if 'choices' in data and len(data['choices']) > 0:
                        answer = data['choices'][0]['message']['content']
                        
                        # è¨˜éŒ„å¯¦éš›ä½¿ç”¨çš„tokenæ•¸
                        usage = data.get('usage', {})
                        actual_tokens = usage.get('total_tokens', 0)
                        logger.info(f"ğŸ“Š å¯¦éš›ä½¿ç”¨tokenæ•¸: {actual_tokens}")
                        
                        return answer
                    else:
                        return "æŠ±æ­‰ï¼ŒAIå›æ‡‰æ ¼å¼ç•°å¸¸ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
                else:
                    error_text = await response.text()
                    logger.error(f"QWEN APIéŒ¯èª¤: {response.status} - {error_text}")
                    return "æŠ±æ­‰ï¼ŒAIæœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
                    
        except Exception as e:
            logger.error(f"èª¿ç”¨QWEN APIå¤±æ•—: {e}")
            return "æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    
    def _extract_patent_references(self, question: str, patents: List[Dict]) -> List[int]:
        """å¾å•é¡Œä¸­æå–å°ˆåˆ©å¼•ç”¨"""
        referenced_patents = []
        
        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åŒ¹é…å°ˆåˆ©åºè™Ÿ
        patterns = [
            r'ç¬¬\s*(\d+)\s*ç­†',
            r'ç¬¬\s*(\d+)\s*å€‹',
            r'åºè™Ÿ\s*(\d+)',
            r'å°ˆåˆ©\s*(\d+)',
            r'(\d+)\s*è™Ÿå°ˆåˆ©',
            r'ç·¨è™Ÿ\s*(\d+)',
            r'å‰\s*(\d+)\s*ç­†',  # æ–°å¢ï¼šå‰Nç­†
            r'æœ€å¾Œ\s*(\d+)\s*ç­†', # æ–°å¢ï¼šæœ€å¾ŒNç­†
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, question)
            for match in matches:
                try:
                    patent_num = int(match)
                    if 1 <= patent_num <= len(patents):
                        referenced_patents.append(patent_num)
                except ValueError:
                    continue
        
        # ç‰¹æ®Šè™•ç†ï¼šå¦‚æœå•åˆ°"é€™äº›å°ˆåˆ©"ã€"æ‰€æœ‰å°ˆåˆ©"ç­‰
        if any(keyword in question for keyword in ['é€™äº›å°ˆåˆ©', 'æ‰€æœ‰å°ˆåˆ©', 'å…¨éƒ¨å°ˆåˆ©', 'æ¯ç­†å°ˆåˆ©']):
            referenced_patents = list(range(1, min(len(patents) + 1, 11)))  # æœ€å¤šå‰10ç­†
        
        # å»é‡ä¸¦æ’åº
        referenced_patents = sorted(list(set(referenced_patents)))
        
        logger.info(f"ğŸ” è­˜åˆ¥åˆ°å¼•ç”¨å°ˆåˆ©: {referenced_patents}")
        return referenced_patents
    
    def _format_patent_for_context(self, patent: Dict, sequence: int) -> str:
        """æ ¼å¼åŒ–å–®å€‹å°ˆåˆ©ç”¨æ–¼ä¸Šä¸‹æ–‡"""
        formatted = f"\nå°ˆåˆ© {sequence}ï¼š\n"
        formatted += f"- åç¨±ï¼š{patent.get('å°ˆåˆ©åç¨±', 'N/A')}\n"
        formatted += f"- å…¬é–‹å…¬å‘Šè™Ÿï¼š{patent.get('å…¬é–‹å…¬å‘Šè™Ÿ', 'N/A')}\n"
        formatted += f"- ç”³è«‹äººï¼š{patent.get('ç”³è«‹äºº', 'N/A')}\n"
        formatted += f"- åœ‹å®¶ï¼š{patent.get('åœ‹å®¶', 'N/A')}\n"
        
        # æˆªå–æ‘˜è¦
        abstract = patent.get('æ‘˜è¦', '')
        if abstract and abstract != 'N/A':
            abstract_short = abstract[:800] + "..." if len(abstract) > 800 else abstract
            formatted += f"- æ‘˜è¦ï¼š{abstract_short}\n"
        
        # æŠ€è¡“ç‰¹å¾µ
        features = patent.get('æŠ€è¡“ç‰¹å¾µ', [])
        if features:
            formatted += f"- æŠ€è¡“ç‰¹å¾µï¼š{'; '.join(features[:5])}\n"
        
        # æŠ€è¡“åŠŸæ•ˆ
        effects = patent.get('æŠ€è¡“åŠŸæ•ˆ', [])
        if effects:
            formatted += f"- æŠ€è¡“åŠŸæ•ˆï¼š{'; '.join(effects[:5])}\n"
        
        return formatted
    
    def _format_patent_brief(self, patent: Dict, sequence: int) -> str:
        """æ ¼å¼åŒ–å°ˆåˆ©ç°¡è¦ä¿¡æ¯"""
        title = patent.get('å°ˆåˆ©åç¨±', 'N/A')
        number = patent.get('å…¬é–‹å…¬å‘Šè™Ÿ', 'N/A')
        return f"{sequence}. {title} ({number})\n"
    
    async def get_conversation_summary(self, session_id: str) -> Dict:
        """ç²å–å°è©±æ‘˜è¦"""
        try:
            history = await DatabaseManager.get_qa_history(session_id, limit=50)
            
            if not history:
                return {
                    'success': True,
                    'summary': 'å°šç„¡å°è©±è¨˜éŒ„',
                    'total_questions': 0
                }
            
            summary = {
                'success': True,
                'total_questions': len(history),
                'first_question_time': history[0]['created_at'] if history else None,
                'last_question_time': history[-1]['created_at'] if history else None,
                'common_topics': self._extract_common_topics(history),
                'recent_questions': [item['question'] for item in history[-5:]]
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"ç²å–å°è©±æ‘˜è¦å¤±æ•—: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _extract_common_topics(self, history: List[Dict]) -> List[str]:
        """æå–å¸¸è¦‹è©±é¡Œ"""
        topics = []
        
        # ç°¡å–®çš„é—œéµè©çµ±è¨ˆ
        keyword_counts = {}
        for item in history:
            question = item['question'].lower()
            
            # æŠ€è¡“ç›¸é—œé—œéµè©
            tech_keywords = ['æŠ€è¡“', 'ç‰¹å¾µ', 'åŠŸæ•ˆ', 'å°ˆåˆ©', 'ç”³è«‹äºº', 'ç¿»è­¯', 'æ¯”è¼ƒ', 'åˆ†æ']
            for keyword in tech_keywords:
                if keyword in question:
                    keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
        
        # è¿”å›å‡ºç¾æ¬¡æ•¸æœ€å¤šçš„å‰3å€‹è©±é¡Œ
        sorted_topics = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)
        topics = [topic for topic, count in sorted_topics[:3] if count > 1]
        
        return topics
    
    async def clear_conversation_memory(self, session_id: str) -> bool:
        """æ¸…é™¤å°è©±è¨˜æ†¶"""
        try:
            # æ¸…é™¤å…§å­˜ç·©å­˜
            if session_id in self.session_conversations:
                del self.session_conversations[session_id]
            
            logger.info(f"å·²æ¸…é™¤æœƒè©± {session_id} çš„å°è©±è¨˜æ†¶")
            return True
            
        except Exception as e:
            logger.error(f"æ¸…é™¤å°è©±è¨˜æ†¶å¤±æ•—: {e}")
            return False

    # ğŸ†• =============== å¤šé‡æœå°‹çµæœæ”¯æ´æ–¹æ³• ===============

    def _determine_target_search_type(self, question: str, available_types: List[str]) -> str:
        """æ™ºèƒ½åˆ¤æ–·ç”¨æˆ¶æƒ³è©¢å•å“ªç¨®æœå°‹çµæœ"""
        question_lower = question.lower()

        # æ˜ç¢ºæŒ‡å®šçš„é—œéµå­—
        if any(keyword in question for keyword in ['æ¢ä»¶æŸ¥è©¢', 'æ¢ä»¶æœå°‹', 'æ¢ä»¶æª¢ç´¢']):
            if 'condition_search' in available_types:
                return 'condition_search'

        if any(keyword in question for keyword in ['æŠ€è¡“æè¿°', 'æŠ€è¡“æŸ¥è©¢', 'æè¿°æŸ¥è©¢']):
            if 'tech_description_search' in available_types:
                return 'tech_description_search'

        if any(keyword in question for keyword in ['excelåˆ†æ', 'excelæª”æ¡ˆ', 'æ‰¹é‡åˆ†æ']):
            if 'excel_analysis' in available_types:
                return 'excel_analysis'

        # å¦‚æœæ²’æœ‰æ˜ç¢ºæŒ‡å®šï¼Œè¿”å›æœ€æ–°çš„æœå°‹é¡å‹
        search_type_priority = ['tech_description_search', 'condition_search', 'excel_analysis']
        for search_type in search_type_priority:
            if search_type in available_types:
                return search_type

        return None

    def _get_search_type_display_name(self, search_type: str) -> str:
        """ç²å–æœå°‹é¡å‹çš„é¡¯ç¤ºåç¨±"""
        type_names = {
            'tech_description_search': 'æŠ€è¡“æè¿°æŸ¥è©¢',
            'condition_search': 'æ¢ä»¶æŸ¥è©¢',
            'excel_analysis': 'Excelåˆ†æ'
        }
        return type_names.get(search_type, search_type)

    def _generate_no_search_results_response(self, question: str) -> str:
        """ç”Ÿæˆæ²’æœ‰æœå°‹çµæœæ™‚çš„å‹å–„å›æ‡‰"""
        if 'ç¿»è­¯' in question:
            return """å¾ˆæŠ±æ­‰ï¼Œæˆ‘ç›®å‰æ²’æœ‰æ‰¾åˆ°ä»»ä½•å°ˆåˆ©è³‡æ–™å¯ä»¥ç¿»è­¯ã€‚

è«‹æ‚¨å…ˆï¼š
1. ğŸ” ä½¿ç”¨ã€ŒæŠ€è¡“æè¿°æŸ¥è©¢ã€åŠŸèƒ½è¼¸å…¥æŠ€è¡“æè¿°ï¼Œè®“æˆ‘æœå°‹ç›¸é—œå°ˆåˆ©
2. ğŸ“‹ æˆ–ä½¿ç”¨ã€Œæ¢ä»¶æŸ¥è©¢ã€åŠŸèƒ½è¨­å®šæœå°‹æ¢ä»¶
3. ğŸ“Š æˆ–ä¸Šå‚³Excelæª”æ¡ˆé€²è¡Œæ‰¹é‡åˆ†æ

å®Œæˆæœå°‹å¾Œï¼Œæˆ‘å°±èƒ½å¹«æ‚¨ç¿»è­¯å’Œåˆ†æå°ˆåˆ©å…§å®¹äº†ï¼æœ‰ä»€éº¼å…¶ä»–å•é¡Œæˆ‘å¯ä»¥å”åŠ©æ‚¨å—ï¼Ÿ"""

        elif 'å°ˆåˆ©' in question:
            return """æ‚¨å¥½ï¼æˆ‘æ³¨æ„åˆ°æ‚¨è©¢å•å°ˆåˆ©ç›¸é—œå…§å®¹ï¼Œä½†ç›®å‰ç³»çµ±ä¸­é‚„æ²’æœ‰å°ˆåˆ©æœå°‹çµæœã€‚

å»ºè­°æ‚¨å¯ä»¥ï¼š
â€¢ ğŸ¯ **æŠ€è¡“æè¿°æŸ¥è©¢**ï¼šæè¿°æ‚¨è¦æ‰¾çš„æŠ€è¡“ï¼Œæˆ‘æœƒç”Ÿæˆé—œéµå­—ä¸¦æœå°‹
â€¢ ğŸ” **æ¢ä»¶æŸ¥è©¢**ï¼šè¨­å®šç”³è«‹äººã€å°ˆåˆ©è™Ÿã€æ™‚é–“ç¯„åœç­‰æ¢ä»¶æœå°‹  
â€¢ ğŸ“ˆ **Excelåˆ†æ**ï¼šä¸Šå‚³å°ˆåˆ©æ¸…å–®é€²è¡Œæ‰¹é‡åˆ†æ

æœå°‹å®Œæˆå¾Œï¼Œæˆ‘å°±èƒ½å›ç­”é—œæ–¼å°ˆåˆ©å…§å®¹ã€æŠ€è¡“ç‰¹å¾µã€ç”³è«‹äººç­‰å„ç¨®å•é¡Œï¼"""

        else:
            return """æ‚¨å¥½ï¼æˆ‘æ˜¯å°ˆåˆ©æª¢ç´¢ç³»çµ±çš„AIåŠ©æ‰‹ã€‚

æˆ‘å¯ä»¥å¹«æ‚¨ï¼š
â€¢ åˆ†æå°ˆåˆ©æŠ€è¡“ç‰¹å¾µå’ŒåŠŸæ•ˆ
â€¢ ç¿»è­¯å°ˆåˆ©å…§å®¹
â€¢ å›ç­”å°ˆåˆ©ç›¸é—œå•é¡Œ
â€¢ å”åŠ©æœå°‹ç­–ç•¥å»ºè­°

è«‹å…ˆåŸ·è¡Œå°ˆåˆ©æœå°‹ï¼Œç„¶å¾Œæˆ‘å°±èƒ½æ ¹æ“šæœå°‹çµæœç‚ºæ‚¨æä¾›è©³ç´°åˆ†æï¼"""

    def _enhance_question_with_search_info(
        self, 
        question: str, 
        available_types: List[str], 
        target_search_type: str,
        results_count: int
    ) -> str:
        """å¢å¼·å•é¡Œï¼ŒåŠ å…¥æœå°‹ä¸Šä¸‹æ–‡ä¿¡æ¯"""

        type_info = []
        for search_type in available_types:
            type_name = self._get_search_type_display_name(search_type)
            type_info.append(type_name)

        enhanced = f"""
ç”¨æˆ¶å•é¡Œï¼š{question}

èƒŒæ™¯ä¿¡æ¯ï¼š
- å¯ç”¨çš„æœå°‹çµæœé¡å‹ï¼š{', '.join(type_info)}
- ä¸»è¦åˆ†æå°è±¡ï¼š{self._get_search_type_display_name(target_search_type) if target_search_type else 'æ‰€æœ‰çµæœ'}
- å°ˆåˆ©ç¸½æ•¸ï¼š{results_count}ç­†

è«‹æ ¹æ“šä¸Šè¿°æœå°‹çµæœå›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚å¦‚æœæ¶‰åŠç‰¹å®šç­†æ•¸ï¼ˆå¦‚"ç¬¬ä¸€ç­†"ã€"ç¬¬äºŒç­†"ï¼‰ï¼Œè«‹ç¢ºä¿æŒ‰ç…§æ­£ç¢ºçš„åºè™Ÿå°æ‡‰ã€‚
"""

        return enhanced

    def _add_source_info_to_answer(
        self, 
        answer: str, 
        available_types: List[str], 
        target_search_type: str,
        results_count: int
    ) -> str:
        """åœ¨å›ç­”å¾ŒåŠ å…¥ä¾†æºä¿¡æ¯"""

        if not available_types:
            return answer

        source_info = "\n\n" + "\n"
        source_info += "**æœå°‹çµæœä¾†æºä¿¡æ¯**\n"
    
        for search_type in available_types:
            type_name = self._get_search_type_display_name(search_type)
            if search_type == target_search_type:
                source_info += f"**{type_name}**ï¼ˆæœ¬æ¬¡å›ç­”åŸºæ–¼æ­¤çµæœï¼‰\n"
            else:
                source_info += f"{type_name}ï¼ˆå¯è©¢å•ç›¸é—œå•é¡Œï¼‰\n"

        source_info += f"\næ‚¨å¯ä»¥è©¢å•ï¼šã€Œ{self._get_search_type_display_name(target_search_type) if target_search_type else 'å…¶ä»–æœå°‹çµæœ'}çš„ç¬¬Xç­†å°ˆåˆ©ã€ä¾†ç²å¾—æ›´ç²¾ç¢ºçš„è³‡è¨Šã€‚"

        return answer + source_info

    def _build_multi_search_context(
        self, 
        question: str, 
        patents: List[Dict], 
        referenced_patents: List[int], 
        target_search_type: str = None
    ) -> str:
        """æ§‹å»ºå¤šé‡æœå°‹çµæœçš„å•ç­”ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æœå°‹é¡å‹æ¨™è¨˜
        results_by_type = {}
        for patent in patents:
            search_type = patent.get('_search_type', 'unknown')
            if search_type not in results_by_type:
                results_by_type[search_type] = []
            results_by_type[search_type].append(patent)
        
        # å¦‚æœåªæœ‰ä¸€ç¨®æœå°‹é¡å‹ï¼Œä½¿ç”¨åŸæœ‰é‚è¼¯
        if len(results_by_type) == 1:
            context_parts.append(f"ä»¥ä¸‹æ˜¯æª¢ç´¢åˆ°çš„ {len(patents)} ç­†å°ˆåˆ©è³‡æ–™ï¼š\n")
            
            # å¦‚æœæœ‰ç‰¹å®šå¼•ç”¨ï¼ŒåªåŒ…å«å¼•ç”¨çš„å°ˆåˆ©
            if referenced_patents:
                context_parts.append("ç‰¹åˆ¥é—œæ³¨ä»¥ä¸‹å°ˆåˆ©ï¼š\n")
                for patent_num in referenced_patents:
                    if 1 <= patent_num <= len(patents):
                        patent = patents[patent_num - 1]
                        context_parts.append(self._format_patent_for_context(patent, patent_num))
            else:
                # å¦‚æœæ²’æœ‰ç‰¹å®šå¼•ç”¨ï¼Œæä¾›æ‰€æœ‰å°ˆåˆ©çš„ç°¡è¦ä¿¡æ¯
                context_parts.append("æ‰€æœ‰å°ˆåˆ©ç°¡è¦ä¿¡æ¯ï¼š\n")
                for i, patent in enumerate(patents[:15]):
                    context_parts.append(self._format_patent_brief(patent, i + 1))
                
                if len(patents) > 15:
                    context_parts.append(f"\n...ï¼ˆé‚„æœ‰ {len(patents) - 15} ç­†å°ˆåˆ©ï¼‰")
        
        else:
            # å¤šç¨®æœå°‹é¡å‹ï¼ŒæŒ‰é¡å‹åˆ†çµ„é¡¯ç¤º
            context_parts.append(f"ä»¥ä¸‹æ˜¯ä¾†è‡ªä¸åŒæœå°‹çš„å°ˆåˆ©è³‡æ–™ï¼ˆå…± {len(patents)} ç­†ï¼‰ï¼š\n")
            
            for search_type, type_patents in results_by_type.items():
                if target_search_type and search_type != target_search_type:
                    continue  # å¦‚æœæŒ‡å®šäº†æœå°‹é¡å‹ï¼Œåªè™•ç†è©²é¡å‹
                
                type_name = self._get_search_type_display_name(search_type)
                context_parts.append(f"\n=== {type_name}çµæœ ({len(type_patents)}ç­†) ===")
                
                # é‡æ–°è¨ˆç®—é€™å€‹é¡å‹ä¸­çš„åºè™Ÿ
                for i, patent in enumerate(type_patents[:10]):  # æ¯ç¨®é¡å‹æœ€å¤šé¡¯ç¤º10ç­†
                    global_index = patents.index(patent) + 1  # åœ¨ç¸½åˆ—è¡¨ä¸­çš„ä½ç½®
                    
                    if referenced_patents and global_index in referenced_patents:
                        context_parts.append(self._format_patent_for_context(patent, global_index))
                    else:
                        context_parts.append(self._format_patent_brief(patent, global_index))
        
        return '\n'.join(context_parts)

# å…¨å±€å¢å¼·ç‰ˆå•ç­”æœå‹™å¯¦ä¾‹
enhanced_patent_qa_service = EnhancedPatentQAService()